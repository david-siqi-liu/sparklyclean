package com.davidsiqiliu.sparklyclean.impl

import org.apache.spark.rdd.RDD

import scala.collection.mutable.{ArrayBuffer, Map}
import scala.util.Random

object Setup {

  def setup(inputFile: RDD[String], k: Int, rand: Random): Map[String, List[Int]] = {
    // Count each block's size generated by all blocking functions
    val sizeByBlock = inputFile
      .flatMap(
        tuple => {
          val bkvs: ArrayBuffer[(String, Int)] = ArrayBuffer()
          for (b <- Util.getBKVs(tuple)) {
            bkvs += ((b, 1))
          }
          bkvs.toList
        }
      )
      .reduceByKey(_ + _)

    // Calculate the total workload
    val numBlocks = sizeByBlock
      .count()

    val workByBlock = sizeByBlock
      .mapValues {
        n => n * (n - 1) / 2
      }

    // Across all blocking functions
    val workTotal = workByBlock
      .map(_._2)
      .sum

    // Partition workByBlock into the following HashMaps based on workload
    //  1. Multi-reducer blocks
    //  2. Single-reducer blocks, larger than tau (for deterministic distribution)
    //  3. Single-reducer blocks, smaller than tau (for randomized distribution)
    val thresholdMultiSingle = workTotal.toDouble / k.toDouble
    val tau = workTotal.toDouble / (3 * k * math.log(k))
    val hmMulti = workByBlock
      .filter {
        case (bkv, w) => w > thresholdMultiSingle
      }
      .collectAsMap()


    val hmSingleD = workByBlock
      .filter {
        case (bkv, w) => w <= thresholdMultiSingle && w > tau
      }
      .sortBy(_._2)
      .collectAsMap()

    val hmSingleR = workByBlock
      .filter {
        case (bkv, w) => w <= tau
      }
      .collectAsMap()

    assert(hmMulti.size + hmSingleD.size + hmSingleR.size == numBlocks)

    // Initialize return HashMap
    val hmBKV2RID = Map[String, List[Int]]()

    // Distribute multi-reducer blocks
    val workMulti = hmMulti.values.sum
    val s = rand.shuffle((1 to k).toList)
    var n = 0
    var k_i = 0
    var l_i = 0
    for ((bkv, w) <- hmMulti) {
      k_i = math.min(w, math.floor(w.toDouble / workMulti.toDouble * k.toDouble).toInt)
      l_i = Util.getL(k_i) // Improvement
      k_i = l_i * (l_i + 1) / 2
      hmBKV2RID(bkv) = s.slice(n, n + k_i)
      n += k_i
    }

    // Distribute large single-reducer blocks, deterministically in a round-robin fashion
    // However, unlike in the paper, we will continue to use the leftover reducers from
    // multi-reducer distribution to avoid un-used reducers
    for ((bkv, w) <- hmSingleD) {
      hmBKV2RID(bkv) = s.slice(n % k, n % k + 1)
      n += 1
    }

    // Distribute small single-reducer blocks, randomly
    for ((bkv, w) <- hmSingleR) {
      hmBKV2RID(bkv) = List(rand.nextInt(k) + 1)
    }

    hmBKV2RID
  }
}
