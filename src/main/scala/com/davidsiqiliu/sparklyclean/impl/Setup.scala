package com.davidsiqiliu.sparklyclean.impl

import com.davidsiqiliu.sparklyclean.impl.Util._
import org.apache.log4j.Logger
import org.apache.spark.rdd.RDD

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer
import scala.util.Random

object Setup {

  def setup(log: Logger, inputFile: RDD[String], k: Int, rand: Random): mutable.Map[BKV, List[Int]] = {
    // Count each block's size generated by all blocking functions
    val sizeByBlock: RDD[(BKV, Long)] = inputFile
      .flatMap(
        tuple => {
          val bkvs: ArrayBuffer[(BKV, Long)] = ArrayBuffer()
          for (b <- getBKVs(tuple)) {
            bkvs += ((b, 1))
          }
          bkvs.toList
        }
      )
      .reduceByKey(_ + _)

    // Calculate the total workload
    val numBlocks: Long = sizeByBlock
      .count()

    val workByBlock: RDD[(BKV, Long)] = sizeByBlock
      .mapValues {
        n => n * (n - 1) / 2
      }

    // Across all blocking functions
    val workTotal: Long = workByBlock
      .map(_._2)
      .sum
      .toLong

    // Partition workByBlock into the following HashMaps based on workload
    //  1. Multi-reducer blocks
    //  2. Single-reducer blocks, larger than tau (for deterministic distribution)
    //  3. Single-reducer blocks, smaller than tau (for randomized distribution)
    val thresholdMultiSingle: Double = workTotal.toDouble / k.toDouble
    val tau: Double = workTotal.toDouble / (3 * k * math.log(k))
    log.info("\nthresholdMultiSingle: " + thresholdMultiSingle.toString)
    log.info("\ntau: " + tau.toString)

    val hmMulti: collection.Map[BKV, Long] = workByBlock
      .filter {
        case (_, w) => w > thresholdMultiSingle
      }
      .collectAsMap()
    log.info("\nhmMulti:\n" + hmMulti.mkString("\n"))

    val hmSingleD: collection.Map[BKV, Long] = workByBlock
      .filter {
        case (_, w) => w <= thresholdMultiSingle && w > tau
      }
      .sortBy(_._2)
      .collectAsMap()
    log.info("\nhmSingleD:\n" + hmSingleD.mkString("\n"))

    val hmSingleR: collection.Map[BKV, Long] = workByBlock
      .filter {
        case (_, w) => w <= tau
      }
      .collectAsMap()
    log.info("\nhmSingleR:\n" + hmSingleR.mkString("\n"))

    assert(hmMulti.size + hmSingleD.size + hmSingleR.size == numBlocks)

    // Initialize return HashMap
    val hmBKV2RID: mutable.Map[BKV, List[Int]] = mutable.Map[BKV, List[Int]]()

    // Distribute multi-reducer blocks
    val workMulti: Long = hmMulti.values.sum
    log.info("\nworkMulti: " + workMulti.toString)

    val hmMultiKi: mutable.Map[BKV, Int] = mutable.Map[BKV, Int]()
    var multiKiDiffsExtras: ArrayBuffer[(BKV, (Int, Int))] = new ArrayBuffer() // (bkv, (diff, extra))
    var k_i_orig: Int = 0
    var k_i: Int = 0
    var k_i_extra: Int = 0
    var l_i: Int = 0

    for ((bkv, w) <- hmMulti) {
      k_i_orig = math.floor(w.toDouble / workMulti.toDouble * k.toDouble).toInt
      l_i = getL(k_i_orig) // Improvement
      k_i = l_i * (l_i + 1) / 2
      k_i_extra = (l_i + 1) * (l_i + 2) / 2
      hmMultiKi(bkv) = k_i
      multiKiDiffsExtras += ((bkv, (k_i_orig - k_i, k_i_extra - k_i)))
    }
    log.info("\nhmMultiKi (prior optimization):\n" + hmMultiKi.mkString("\n"))

    multiKiDiffsExtras = multiKiDiffsExtras
      .filter(_._2._1 > 0)
      .sortBy(_._2._1)(Ordering[Int].reverse)

    log.info("\nmultiKiDiffsExtras:\n" + multiKiDiffsExtras.mkString("\n"))

    var numKLeftovers: Int = k - hmMultiKi.values.sum
    for ((bkv, (_, extra)) <- multiKiDiffsExtras) {
      if (extra <= numKLeftovers) {
        hmMultiKi(bkv) += extra
        numKLeftovers -= extra
      }
    }
    log.info("\nhmMultiKi (after optimization):\n" + hmMultiKi.mkString("\n"))

    val s: List[Int] = rand.shuffle((1 to k).toList)
    log.info("\nShuffled s: " + s.toString())

    var n: Int = 0
    for ((bkv, k_i) <- hmMultiKi) {
      hmBKV2RID(bkv) = s.slice(n, n + k_i)
      n += k_i
    }

    // Distribute large single-reducer blocks, deterministically in a round-robin fashion
    // However, unlike in the paper, we will continue to use the leftover reducers from
    // multi-reducer distribution to avoid un-used reducers
    for ((bkv, _) <- hmSingleD) {
      hmBKV2RID(bkv) = s.slice(n % k, n % k + 1)
      n += 1
    }

    // Distribute small single-reducer blocks, randomly
    for ((bkv, _) <- hmSingleR) {
      hmBKV2RID(bkv) = List(rand.nextInt(k) + 1)
    }

    hmBKV2RID
  }
}
